{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rycg_nuWQFRq"
      },
      "outputs": [],
      "source": [
        "!pip install opacus==1.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHvZM_qCQfhf",
        "outputId": "f3caec21-3a57-445b-86e9-b5b706464b48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 84.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from typing import Callable, Dict, Tuple\n",
        "import numpy as np\n",
        "from opacus import GradSampleModule\n",
        "from opacus import PrivacyEngine\n",
        "from dp_optimizer import DPMFSGD\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "# Define the split sizes\n",
        "train_size = int(0.8 * len(full_train_dataset))  # 80% for training\n",
        "test_size = len(full_train_dataset) - train_size  # 20% for testing\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Define CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # First block: Conv(32) -> ReLU -> Conv(32) -> ReLU -> MaxPool\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second block: Conv(64) -> ReLU -> Conv(64) -> ReLU -> MaxPool\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Third block: Conv(128) -> ReLU -> Conv(128) -> ReLU -> MaxPool\n",
        "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten layer\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Fully connected layer (Dense Layer): 128 * 4 * 4 -> 10 classes\n",
        "        self.fc = nn.Linear(128 * 4 * 4, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First block\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Second block\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Third block\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Fully connected layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wKrq7hxQJlt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "def train(model, train_dataset, criterion, optimizer, epochs, batch_size, device):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "      for inputs, targets in train_loader:\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total_samples = inputs.size(0)\n",
        "\n",
        "        microbatch_size = 256\n",
        "\n",
        "        # Process in microbatches\n",
        "        for start in range(0, total_samples, microbatch_size):\n",
        "          end = start + microbatch_size\n",
        "          micro_inputs = inputs[start:end]\n",
        "          micro_targets = targets[start:end]\n",
        "\n",
        "          optimizer.zero_microbatch_grad()\n",
        "          outputs = model(micro_inputs)\n",
        "          loss = criterion(outputs, micro_targets)\n",
        "          loss.backward()\n",
        "          optimizer.microbatch_step()\n",
        "\n",
        "        # Step update after all microbatches\n",
        "        optimizer.step()\n",
        "\n",
        "      # Evaluate on test set after each epoch\n",
        "      test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "      print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "      print('___________________________________________________________')\n",
        "\n",
        "# Test function\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    running_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader)\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_xtG6nfQ-rh"
      },
      "outputs": [],
      "source": [
        "def run_experiments(num_iterations=100, b_min_sep=10, epochs=10, clip_norm=1, lr=0.001, momentum=0, weight_decay=0.999, sigma=1, batch_size=512,  bandwidth=5, factorization_type='band', MF_coef=None, use_amplification=False):\n",
        "    model = GradSampleModule(CNNModel())\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize DP-MF-SGD optimizer\n",
        "    optimizer = DPMFSGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, noise_multiplier=sigma, l2_norm_clip=clip_norm, batch_size=batch_size,\n",
        "                        iterations_number=num_iterations, b_min_sep=b_min_sep, band_width=bandwidth, device=device, factorization_type=factorization_type, MF_coef=MF_coef, use_amplification=use_amplification)\n",
        "\n",
        "    train(model, train_dataset, criterion, optimizer, epochs, batch_size, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCFubeY2RJVw",
        "outputId": "97b6fd25-aab4-42bc-e06f-d848029c168d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "C matrix sensitivity 3.316624879837036\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.2960, Test Accuracy: 12.17%\n",
            "___________________________________________________________\n",
            "Epoch 2/10\n",
            "Test Loss: 2.0781, Test Accuracy: 23.57%\n",
            "___________________________________________________________\n",
            "Epoch 3/10\n",
            "Test Loss: 1.9734, Test Accuracy: 29.70%\n",
            "___________________________________________________________\n",
            "Epoch 4/10\n",
            "Test Loss: 1.9176, Test Accuracy: 33.21%\n",
            "___________________________________________________________\n",
            "Epoch 5/10\n",
            "Test Loss: 1.9300, Test Accuracy: 32.95%\n",
            "___________________________________________________________\n",
            "Epoch 6/10\n",
            "Test Loss: 1.9598, Test Accuracy: 33.64%\n",
            "___________________________________________________________\n",
            "Epoch 7/10\n",
            "Test Loss: 2.0206, Test Accuracy: 34.71%\n",
            "___________________________________________________________\n",
            "Epoch 8/10\n",
            "Test Loss: 2.0254, Test Accuracy: 33.81%\n",
            "___________________________________________________________\n",
            "Epoch 9/10\n",
            "Test Loss: 2.0145, Test Accuracy: 33.82%\n",
            "___________________________________________________________\n",
            "Epoch 10/10\n",
            "Test Loss: 2.0684, Test Accuracy: 34.14%\n",
            "___________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\"\"\" DP-SGD \"\"\"\n",
        "\n",
        "sigma = 0.54 # for epsilon = 9, delta = 10^{-5}\n",
        "epoch = 10\n",
        "batch_size = 512\n",
        "num_iterations = epoch * len(train_dataset) // batch_size\n",
        "b_min_sep = num_iterations // epoch\n",
        "run_experiments(num_iterations=num_iterations, b_min_sep=b_min_sep, epochs=epoch, lr=0.1, momentum=0.9, weight_decay=0.9999, clip_norm=10, sigma=sigma, batch_size=batch_size, bandwidth=1, use_amplification=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDcix43fQ_hh",
        "outputId": "d81bd302-0bed-4b23-fc38-2b1be42e2dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "C matrix sensitivity 5.966540813446045\n",
            "Epoch 1/10\n",
            "Test Loss: 2.2929, Test Accuracy: 14.99%\n",
            "___________________________________________________________\n",
            "Epoch 2/10\n",
            "Test Loss: 2.0144, Test Accuracy: 28.41%\n",
            "___________________________________________________________\n",
            "Epoch 3/10\n",
            "Test Loss: 1.9006, Test Accuracy: 33.55%\n",
            "___________________________________________________________\n",
            "Epoch 4/10\n",
            "Test Loss: 1.9029, Test Accuracy: 36.09%\n",
            "___________________________________________________________\n",
            "Epoch 5/10\n",
            "Test Loss: 1.9725, Test Accuracy: 36.82%\n",
            "___________________________________________________________\n",
            "Epoch 6/10\n",
            "Test Loss: 1.9468, Test Accuracy: 36.74%\n",
            "___________________________________________________________\n",
            "Epoch 7/10\n",
            "Test Loss: 1.9629, Test Accuracy: 37.88%\n",
            "___________________________________________________________\n",
            "Epoch 8/10\n",
            "Test Loss: 1.9171, Test Accuracy: 39.89%\n",
            "___________________________________________________________\n",
            "Epoch 9/10\n",
            "Test Loss: 1.9655, Test Accuracy: 38.00%\n",
            "___________________________________________________________\n",
            "Epoch 10/10\n",
            "Test Loss: 1.9018, Test Accuracy: 39.60%\n",
            "___________________________________________________________\n"
          ]
        }
      ],
      "source": [
        " \"\"\" BSR \"\"\"\n",
        "\n",
        "sigma = 0.54 # for epsilon = 9, delta = 10^{-5}\n",
        "epoch = 10\n",
        "batch_size = 512\n",
        "num_iterations = epoch * len(train_dataset) // batch_size\n",
        "b_min_sep = num_iterations // epoch\n",
        "run_experiments(num_iterations=num_iterations, b_min_sep=b_min_sep, epochs=epoch, lr=2e-1, momentum=0.9, weight_decay=0.9999, clip_norm=10, sigma=sigma, batch_size=batch_size, bandwidth=4, use_amplification=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9_Sl5ZA-RG7S",
        "outputId": "001b82cc-ac01-4db8-9a70-2ed1641f5b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "C matrix sensitivity 10.614968299865723\n",
            "Epoch 1/10\n",
            "Test Loss: 2.2816, Test Accuracy: 20.99%\n",
            "___________________________________________________________\n",
            "Epoch 2/10\n",
            "Test Loss: 1.9136, Test Accuracy: 33.95%\n",
            "___________________________________________________________\n",
            "Epoch 3/10\n",
            "Test Loss: 1.7913, Test Accuracy: 38.79%\n",
            "___________________________________________________________\n",
            "Epoch 4/10\n",
            "Test Loss: 1.7861, Test Accuracy: 40.18%\n",
            "___________________________________________________________\n",
            "Epoch 5/10\n",
            "Test Loss: 1.8357, Test Accuracy: 41.74%\n",
            "___________________________________________________________\n",
            "Epoch 6/10\n",
            "Test Loss: 1.6558, Test Accuracy: 45.32%\n",
            "___________________________________________________________\n",
            "Epoch 7/10\n",
            "Test Loss: 1.7573, Test Accuracy: 43.94%\n",
            "___________________________________________________________\n",
            "Epoch 8/10\n",
            "Test Loss: 1.6195, Test Accuracy: 47.04%\n",
            "___________________________________________________________\n",
            "Epoch 9/10\n",
            "Test Loss: 1.7252, Test Accuracy: 47.31%\n",
            "___________________________________________________________\n",
            "Epoch 10/10\n",
            "Test Loss: 1.5738, Test Accuracy: 48.61%\n",
            "___________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\"\"\" BISR \"\"\"\n",
        "\n",
        "sigma = 0.54 # for epsilon = 9, delta = 10^{-5}\n",
        "epoch = 10\n",
        "batch_size = 512\n",
        "num_iterations = epoch * len(train_dataset) // batch_size\n",
        "b_min_sep = num_iterations // epoch\n",
        "run_experiments(num_iterations=num_iterations, b_min_sep=b_min_sep, epochs=epoch, lr=0.7, momentum=0.9, weight_decay=0.9999, clip_norm=10, sigma=sigma, batch_size=batch_size, bandwidth=4, factorization_type='band-inv', use_amplification=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1i4Bf63XaDR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}